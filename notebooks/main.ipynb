{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages to Import\n",
    "from threading import Thread\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import fftconvolve\n",
    "import IPython\n",
    "import pyroomacoustics as pra\n",
    "import csv\n",
    "from itertools import combinations\n",
    "import scipy.io as sio\n",
    "from scipy import spatial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreadWithReturnValue(Thread):\n",
    "    \"\"\" \n",
    "    Created a Thread subclass. It is a workable doaround,\n",
    "    but it accesses \"private\" data structures that are specific to Thread implementation, so \n",
    "    things will get a little hairy. \"\"\"\n",
    "    \n",
    "    def __init__(self, group=None, target=None, name=None,args=(), kwargs={}, Verbose=None):\n",
    "        \"\"\" Initializes the thread object. \"\"\"\n",
    "        \n",
    "        Thread.__init__(self, group, target, name, args, kwargs)\n",
    "        self._return = None\n",
    "        \n",
    "    def run(self):\n",
    "        \"\"\" Runs the function if specified for the thread. \"\"\"\n",
    "        # If the target function is specified\n",
    "        if self._target is not None:\n",
    "            \n",
    "            # Run the function \n",
    "            self._return = self._target(*self._args, **self._kwargs)\n",
    "            \n",
    "    def join(self, *args):\n",
    "        \"\"\" Returns the value of target function running in the thread. \"\"\"\n",
    "        \n",
    "        Thread.join(self, *args)\n",
    "        return self._return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(source_name):\n",
    "    \n",
    "    \"\"\" Returns the data depending on the cycle number related and the specific sound source.\n",
    "    \n",
    "        Keyword arguments:\n",
    "        \n",
    "            source_name -- specifies each sound cycle\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    # Create a dictionary with all cycles: Regular S1 and S1 (top) Recovered S1 and S2 (bottom)\n",
    "    source_name_dict = {f'S{x}_Cycle{y}': [f'S{x}/S{x}_Cycle{y}', f'S{x}'] for x in range(1,3) for y in range(24)} \n",
    "    #source_name_dict = {f'S{x}_Cycle{y}': [f'Recovered_S{x}/S{x}_Cycle{y}', f'S{x}'] for x in range(1,3) for y in range(24)}\n",
    "    \n",
    "    # Match the correct data with the name\n",
    "    for key in source_name_dict.keys():\n",
    "        if source_name == key:\n",
    "            data = sio.loadmat(source_name_dict[key][0])\n",
    "            sound_data = data[source_name_dict[key][1]]\n",
    "    \n",
    "    # Return the sound data\n",
    "    return sound_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid(*args):\n",
    "    \"\"\" Returns the center of n number of microphones.\n",
    "\n",
    "    Keyword Arguments:\n",
    "\n",
    "        args -- location of each n microphone\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Initiate\n",
    "    microphone_array = np.zeros((len(args), len(args[1])))\n",
    "\n",
    "    # Converts microphone locations into an array\n",
    "    for i, _ in enumerate(args):\n",
    "        microphone_array[i, :] = np.array(args[i])\n",
    "\n",
    "    # Finds the centroid\n",
    "    return np.sum(microphone_array, axis=0) / len(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_microphone_locations_array():\n",
    "    \n",
    "    \"\"\" Helper function designed to create microphone locations array. \"\"\"\n",
    "    \n",
    "    # Microphone x,y,z locations\n",
    "    x_locations = [-0.102235, -0.052197, -0.027304]\n",
    "    y_locations = [-0.109982]\n",
    "    z_locations = [0.056388, 0.001524, -0.053340, -0.108204]\n",
    "\n",
    "    # Create the microphone array\n",
    "    microphone_locations = [[x,y,z] for x in x_locations for y in y_locations for z in z_locations]\n",
    "    \n",
    "    return microphone_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mic_run(data, *args):\n",
    "    \n",
    "    \"\"\" Returns each of the n microphone locations and the signals list corresponding to the specific microphone.\n",
    "        Note: The microphone locations are under a new coordinate system in relation to the center of the box\n",
    "              (whose center = [(0.34925/2),(0.219964/2),(0.2413/2)] is the origin)\n",
    "    \n",
    "        Keyword arguments:\n",
    "        \n",
    "            data -- the signal associated with each microphone\n",
    "            args -- list of the microphones \n",
    "    \"\"\"\n",
    "    \n",
    "    # Empty Lists\n",
    "    signal_list = []\n",
    "    mic_location = []\n",
    "    \n",
    "    # Get the microphone locations array\n",
    "    microphone_locations = create_microphone_locations_array()\n",
    "    \n",
    "    # List of Microphone array and data\n",
    "    microphone_locations_and_data = list(zip(microphone_locations, (row for row in data)))\n",
    "    \n",
    "    # Dictionary of the microphone locations and their respective signals\n",
    "    # Note: order is #mic number (from 1 -12), followed by location of channel (to get actual signal)\n",
    "    microphones_locations_dict = {'mic'+str(j+1): \n",
    "                                  microphone_locations_and_data[j] for j in range(len(microphone_locations_and_data))}\n",
    "    \n",
    "    # Look for a match between the dictionary of microphone locations and the microphone in the list\n",
    "    for arg in args:\n",
    "        for key in microphones_locations_dict.keys():\n",
    "            if arg == key:\n",
    "                # Record the location\n",
    "                mic_location.append(microphones_locations_dict[key][0])\n",
    "                \n",
    "                # Record the signal\n",
    "                signal_list.append(microphones_locations_dict[key][1])\n",
    "    \n",
    "   # Return the whole signal list as well all the specific microphone locations\n",
    "    return signal_list, mic_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_of_arrivals(speed_sound,signal_list,algo_name,num_sources,*mic_location):\n",
    "\n",
    "    \"\"\" Returns an azimuth and co-latitude for each pair of microphones. \n",
    "\n",
    "        Keyword arugments:\n",
    "\n",
    "            sound_speed -- specific speed of sound\n",
    "            signal_list -- the microphone signals\n",
    "            algo_name -- specific distance of arrival (DOA) method\n",
    "            num_sources -- number of Sourcs to find\n",
    "            mic_location -- location of each microphone\n",
    "    \"\"\"\n",
    "    \n",
    "    # Constants \n",
    "    fs = 16000  # sampling frequency\n",
    "    nfft = 256  # FFT size\n",
    "    \n",
    "    # Add 3-microphone array in [x,y,z] order\n",
    "    R = np.vstack(list(zip(*mic_location)))\n",
    "    \n",
    "    # Create an array of a short fourier transformed frequency signal\n",
    "    X = np.array([pra.stft(signal, nfft, nfft//2,transform=np.fft.rfft).T for signal in signal_list])\n",
    "    \n",
    "    # Frequency Range\n",
    "    freq_range = [0,250]\n",
    "    \n",
    "    # Construct the new DOA object\n",
    "    doa = pra.doa.algorithms[algo_name](L=R, fs=fs, nfft=nfft, c=speed_sound, num_src=num_sources, max_four=4,\n",
    "    dim=3,azimuth=np.linspace(-180.,180.,360)*np.pi/180,\n",
    "    colatitude=np.linspace(-90.,90.,180)*np.pi/180)\n",
    "    \n",
    "    # Locate the sources\n",
    "    doa.locate_sources(X, freq_range=freq_range)\n",
    "    \n",
    "    # Return all in radians\n",
    "    return doa.azimuth_recon, doa.colatitude_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_conquer(radius, centroid, arr, num_sources):\n",
    "    \"\"\" Helper function used when looking for multiple sources. Breaks up the cartesian array into smaller bits\n",
    "        in order to help multiply the radius and recenter. \n",
    "        \n",
    "        Keyword arguements:\n",
    "            \n",
    "            radius -- set of points we are using for our radius\n",
    "            centroid -- the specific centroid associated with each coordinate\n",
    "            arr -- the cartesian array before the multiplication of the radius\n",
    "            num_sources -- number of sources we are looking for\n",
    "        \"\"\"\n",
    "    \n",
    "    # Check if the cartesian array is a numpy array\n",
    "    if isinstance(arr, np.ndarray):\n",
    "        \n",
    "        # Split up the array into the seperate parts based on how many sources there are\n",
    "        array_split = np.vsplit(arr.T, num_sources)\n",
    "        \n",
    "        # Multiply each respective part by the radius and recenter it with the centroid\n",
    "        for i in range(num_sources):\n",
    "            array_split[i] = radius*array_split[i] + np.array(centroid)[np.newaxis,:]\n",
    "        \n",
    "        # Multiply each respective part by the radius and recenter it with the centroid\n",
    "#         new_arr_1 = radius*arr_part_1 + np.array(centroid)[np.newaxis,:]\n",
    "#         new_arr_2 = radius*arr_part_2 + np.array(centroid)[np.newaxis,:] \n",
    "    \n",
    "    # Return the new array\n",
    "#     return np.vstack((new_arr_1, new_arr_2))\n",
    "    return np.vstack((array_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(sound_speed,algo_name,sound_data,combinations_number,S1_bool,source_name,num_sources=2):\n",
    "    \n",
    "    \"\"\" Returns list of points that are either close to the point or the exact point itself.\n",
    "    \n",
    "        Keyword arguments:\n",
    "        \n",
    "            sound_speed -- specific speed of sound\n",
    "            algo_name -- specific distance of arrival (DOA) method to call\n",
    "            sound_data -- specific data to perform the localizing\n",
    "            combinations_number -- number of microphone to use\n",
    "            S1_bool -- flag to indicate whether to find S1 or S2 sound source\n",
    "            source_name -- indicates S1 or S2 and what specific cycle\n",
    "            num_source -- number of sources to find. Default is 1\n",
    "    \"\"\"\n",
    "    \n",
    "    # Optimization:\n",
    "    if S1_bool:\n",
    "        # List of specific microphones to quickly find S1 (where M and T are located)\n",
    "        mics = ['mic'+str(i) for i in [2, 3, 6, 7, 10, 11]]\n",
    "        \n",
    "    else:\n",
    "        # List of specific microphones to quickly find S2 (where P and A are located)\n",
    "        mics = ['mic'+str(i) for i in [1, 2 ,5, 6, 9, 10]] \n",
    "  \n",
    "    # Creates a list of N microphone-combinations\n",
    "    mic_list=list(combinations(mics,combinations_number))\n",
    "    \n",
    "    # number of mic pair splits to run \n",
    "    splits = len(mic_list)//5\n",
    "    \n",
    "    # Split up the mic list into chunks of the same size\n",
    "    mic_split_list = [mic_list[i*splits:(i+1)*splits] for i in range((len(mic_list)+splits-1)//splits)]\n",
    "    \n",
    "    # Store the final output\n",
    "    outputs_list = []\n",
    "\n",
    "    # Constants: Tolerance and Radius\n",
    "    tol = 3e-3\n",
    "    r = np.arange(0,0.5,tol)[:, np.newaxis]\n",
    "    \n",
    "    # Go through all the chunks in the multi-thread\n",
    "    for j in range(splits):\n",
    "              \n",
    "        # Pick the signal array and the associated pair of n microphone combinations\n",
    "        # Note: Multi-threaded the mic_run function for faster use\n",
    "        twrv1 = ThreadWithReturnValue(target=mic_run, args=(sound_data, *mic_split_list[0][j]))\n",
    "        twrv2 = ThreadWithReturnValue(target=mic_run, args=(sound_data, *mic_split_list[1][j]))\n",
    "        twrv3 = ThreadWithReturnValue(target=mic_run, args=(sound_data, *mic_split_list[2][j]))\n",
    "        twrv4 = ThreadWithReturnValue(target=mic_run, args=(sound_data, *mic_split_list[3][j]))\n",
    "        twrv5 = ThreadWithReturnValue(target=mic_run, args=(sound_data, *mic_split_list[4][j]))\n",
    "    \n",
    "        # Start the multi-thread\n",
    "        twrv1.start()\n",
    "        twrv2.start()\n",
    "        twrv3.start()\n",
    "        twrv4.start()\n",
    "        twrv5.start()\n",
    "        \n",
    "        # Return signal and microphone locations\n",
    "        [signal_1, mic_locations_1] = twrv1.join()\n",
    "        [signal_2, mic_locations_2] = twrv2.join()\n",
    "        [signal_3, mic_locations_3] = twrv3.join()\n",
    "        [signal_4, mic_locations_4] = twrv4.join()\n",
    "        [signal_5, mic_locations_5] = twrv5.join()\n",
    "   \n",
    "        # Find the centriods of the pairs of n microphone combinations\n",
    "        twrv7 = ThreadWithReturnValue(target=centroid, args=(mic_locations_1))\n",
    "        twrv8 = ThreadWithReturnValue(target=centroid, args=(mic_locations_2))\n",
    "        twrv9 = ThreadWithReturnValue(target=centroid, args=(mic_locations_3))\n",
    "        twrv10 = ThreadWithReturnValue(target=centroid, args=(mic_locations_4))\n",
    "        twrv11 = ThreadWithReturnValue(target=centroid, args=(mic_locations_5))\n",
    "\n",
    "        twrv7.start()\n",
    "        twrv8.start()\n",
    "        twrv9.start()\n",
    "        twrv10.start()\n",
    "        twrv11.start()\n",
    "        \n",
    "        # Return the centeriods\n",
    "        centroid_1 = twrv7.join()\n",
    "        centroid_2 = twrv8.join()\n",
    "        centroid_3 = twrv9.join()\n",
    "        centroid_4 = twrv10.join()\n",
    "        centroid_5 = twrv11.join()\n",
    "        \n",
    "        # Perform the distance of arrival methods to find closest azimuth and colatitude angles\n",
    "        twrv13 = ThreadWithReturnValue(target=difference_of_arrivals, \n",
    "                                       args=(sound_speed,signal_1,algo_name,num_sources,*mic_locations_1))\n",
    "        twrv14 = ThreadWithReturnValue(target=difference_of_arrivals, \n",
    "                                       args=(sound_speed,signal_2,algo_name,num_sources,*mic_locations_2))\n",
    "        twrv15 = ThreadWithReturnValue(target=difference_of_arrivals, \n",
    "                                       args=(sound_speed,signal_3,algo_name,num_sources,*mic_locations_3))\n",
    "        twrv16 = ThreadWithReturnValue(target=difference_of_arrivals, \n",
    "                                       args=(sound_speed,signal_4,algo_name,num_sources,*mic_locations_4))\n",
    "        twrv17 = ThreadWithReturnValue(target=difference_of_arrivals, \n",
    "                                       args=(sound_speed,signal_5,algo_name,num_sources,*mic_locations_5))\n",
    "                \n",
    "        twrv13.start()\n",
    "        twrv14.start()\n",
    "        twrv15.start()\n",
    "        twrv16.start()\n",
    "        twrv17.start()\n",
    "\n",
    "        # Desired angles \n",
    "        azimuth_recon_1, colatitude_recon_1 = twrv13.join()\n",
    "        azimuth_recon_2, colatitude_recon_2 = twrv14.join()\n",
    "        azimuth_recon_3, colatitude_recon_3 = twrv15.join()\n",
    "        azimuth_recon_4, colatitude_recon_4 = twrv16.join()\n",
    "        azimuth_recon_5, colatitude_recon_5 = twrv17.join()\n",
    "               \n",
    "        # Desired cartesian coordinates\n",
    "        cartesian_1 = np.array([np.cos(azimuth_recon_1)*np.sin(colatitude_recon_1),\n",
    "                                np.sin(azimuth_recon_1)*np.sin(colatitude_recon_1),np.cos(colatitude_recon_1)])\n",
    "        cartesian_2 = np.array([np.cos(azimuth_recon_2)*np.sin(colatitude_recon_2),\n",
    "                                np.sin(azimuth_recon_2)*np.sin(colatitude_recon_2),np.cos(colatitude_recon_2)])\n",
    "        cartesian_3 = np.array([np.cos(azimuth_recon_3)*np.sin(colatitude_recon_3),\n",
    "                                np.sin(azimuth_recon_3)*np.sin(colatitude_recon_3),np.cos(colatitude_recon_3)])\n",
    "        cartesian_4 = np.array([np.cos(azimuth_recon_4)*np.sin(colatitude_recon_4),\n",
    "                                np.sin(azimuth_recon_4)*np.sin(colatitude_recon_4),np.cos(colatitude_recon_4)])\n",
    "        cartesian_5 = np.array([np.cos(azimuth_recon_5)*np.sin(colatitude_recon_5),\n",
    "                                np.sin(azimuth_recon_5)*np.sin(colatitude_recon_5),np.cos(colatitude_recon_5)])\n",
    "        \n",
    "        if num_sources > 1:\n",
    "            # Get the estimates \n",
    "            estimate_1 = split_and_conquer(r, centroid_1, cartesian_1, num_sources)\n",
    "            estimate_2 = split_and_conquer(r, centroid_2, cartesian_2, num_sources)\n",
    "            estimate_3 = split_and_conquer(r, centroid_3, cartesian_3, num_sources)\n",
    "            estimate_4 = split_and_conquer(r, centroid_4, cartesian_4, num_sources)\n",
    "            estimate_5 = split_and_conquer(r, centroid_5, cartesian_5, num_sources)\n",
    "        else:\n",
    "            # Re-center them via adding the centroid        \n",
    "            estimate_1 = r*cartesian_1.T + np.array(centroid_1)[np.newaxis,:] \n",
    "            estimate_2 = r*cartesian_2.T + np.array(centroid_2)[np.newaxis,:]\n",
    "            estimate_3 = r*cartesian_3.T + np.array(centroid_3)[np.newaxis,:] \n",
    "            estimate_4 = r*cartesian_4.T + np.array(centroid_4)[np.newaxis,:]\n",
    "            estimate_5 = r*cartesian_5.T + np.array(centroid_5)[np.newaxis,:]\n",
    "                \n",
    "        # Add to an output list\n",
    "        outputs_list.extend((estimate_1, estimate_2, estimate_3, estimate_4, estimate_5))\n",
    "    \n",
    "    # Make a numpy array of them \n",
    "    all_estimates = np.array(outputs_list)\n",
    "        \n",
    "    # Reshape them to (_, 3) which is proper format for the tree\n",
    "    total_array = np.reshape(all_estimates,(all_estimates.shape[0]*all_estimates.shape[1], all_estimates.shape[2]))\n",
    "    \n",
    "    # Re-center the points, add the x,y,z location of the center of the room to the obtained point\n",
    "    room_dim = np.array([0.34925,0.219964,0.2413]) #[13.75,8.66,9.5] # Width, Depth, Length\n",
    "    centerlist = (room_dim)/2\n",
    "    \n",
    "    # Reconvert all the potential source points\n",
    "    potential_sources = np.add(centerlist, total_array) # Width, Depth, Length\n",
    "    \n",
    "    if S1_bool:\n",
    "        # Set the boundaries on where we think S1 lies\n",
    "        source = potential_sources[(potential_sources[:,0] >= 0.07) & (potential_sources[:,0] < 0.15)\n",
    "                                   & (potential_sources[:,1] > 8e-2) & (potential_sources[:,1] <= 0.10) #room_dim[1] \n",
    "                                   & (potential_sources[:,2] >= 0.06) & (potential_sources[:,2] < 0.12)]\n",
    "\n",
    "        # Recenter the S1 source\n",
    "        S_source = np.add(centerlist, np.array([-0.0639405 , -0.01994509, -0.02030148]))\n",
    "        \n",
    "    else:\n",
    "        # Set the boundaries on where we think S2 lies\n",
    "        source = potential_sources[(potential_sources[:,0] >= 0.07) & (potential_sources[:,0] < 0.15)\n",
    "                                  & (potential_sources[:,1] > 0.065) & (potential_sources[:,1] <= 0.095)\n",
    "                                  & (potential_sources[:,2] >= 0.12) & (potential_sources[:,2] < 0.18)]\n",
    "        \n",
    "         # Recenter the S2 Source\n",
    "        # S_source = np.add(centerlist, np.array([-0.09080822, -0.03022343,  0.02206185]))\n",
    "        S_source = np.array([0.10, 0.07975857, 0.14271185])\n",
    "    \n",
    "    # Are there are more than 1 sources?\n",
    "    if source.size > 0:                      \n",
    "\n",
    "        microphone_locations = create_microphone_locations_array()\n",
    "\n",
    "        # Add the locations\n",
    "        microphone_source_locations = np.add(centerlist, np.array(microphone_locations))\n",
    "\n",
    "        # Create a Figure, label the axis, Title the plot, and set the limits\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.set_xlabel('Width (X axis)')\n",
    "        ax.set_ylabel('Depth (Z axis)')\n",
    "        ax.set_zlabel('Length (Y axis)')\n",
    "        ax.set_title(\"All the Clusters\")\n",
    "        ax.set_xlim(0, 0.35)\n",
    "        ax.set_ylim(0, 0.25)\n",
    "        ax.set_zlim(0, 0.22) # for 3-d\n",
    "\n",
    "        # Plot the microphones\n",
    "        ax.scatter(microphone_source_locations[:,0], microphone_source_locations[:,1], \n",
    "                   microphone_source_locations[:,2], label='Microphones 1-12')\n",
    "\n",
    "        # Plot the S1 or S2 location\n",
    "        ax.scatter(S_source[0], S_source[1], S_source[2], 'b', label='Source Location')\n",
    "\n",
    "        # Plot all the possible S1 or S2 sources\n",
    "        ax.scatter(source[:, 0], source[:, 1], source[:, 2], 'y', label='Potential Source Location')\n",
    "\n",
    "        ax.legend()\n",
    "\n",
    "        # Write to a csv file to save the data\n",
    "        filename = 'mic_'+str(combinations_number)+'_'+str(source_name)+'_sound_source_localization_c'+str(sound_speed)+'_'+str(algo_name)+'_CLUSTER_multithread'+str(num_sources)\n",
    "\n",
    "        # Save the file\n",
    "        fig.savefig(filename+'.png')\n",
    "        plt.close(fig)\n",
    "\n",
    "        with open(filename+'.csv', mode='w') as sound_source_file:\n",
    "            writer = csv.writer(sound_source_file,delimiter=',')\n",
    "\n",
    "            # First Row of Data, names of the columns\n",
    "            writer.writerow(['Width', 'Depth', 'Length'])\n",
    "\n",
    "            # Write the rest of the results\n",
    "            # Note they have not been converted back into correct x,y,z coordinates\n",
    "            writer.writerows(source) \n",
    "\n",
    "        sound_source_file.close()\n",
    "        \n",
    "    else:\n",
    "        print(\"Nothing to convert. Points do not exist inside the boundaries of the environment for \"+str(source_name)+\"_\"+str(algo_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_KD_tree(S1_bool, centerlist, total_array):\n",
    "    \"\"\" Optional: Use the KD Tree Structure to find S1 and S2 sources. \n",
    "        \n",
    "        Keyword:\n",
    "            S1_bool -- boolean to determine whether to track for S1 or S2 sound source\n",
    "            centerlist -- center of the room\n",
    "            total_array -- the array of the possible sources. \n",
    "             \n",
    "        Runtime Complexity:\n",
    "            Best Case: O(log(n))\n",
    "            Worst Case: O(n)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Put the whole list into a tree data data structure\n",
    "    tree = spatial.KDTree(total_array)\n",
    "    \n",
    "    if S1_bool:\n",
    "        # Find the points closest to where S1 sound is\n",
    "        S1_indices = tree.query_ball_point([-0.0639405 , -0.01994509, -0.02030148], 2.5e-2)\n",
    "        potential_sources = np.array([total_array[S1_indices][j] for j in range(len(S1_indices))])\n",
    "        \n",
    "        # Reconvert all the potential source points\n",
    "        source = np.add(centerlist, potential_sources) # Width, Depth, Length\n",
    "        \n",
    "        # Recenter the S1 source\n",
    "        S_source = np.add(centerlist, np.array([-0.0639405 , -0.01994509, -0.02030148]))\n",
    "        \n",
    "        # return the potential S1 source and the S1 source\n",
    "        return source, S_source\n",
    "        \n",
    "    # Find the point closest to where S2 sound is\n",
    "    S2_indices = tree.query_ball_point([-0.09080822, -0.03022343,  0.02206185], 2.5e-2)\n",
    "    potential_sources = np.array([total_array[S2_indices][j] for j in range(len(S2_indices))])\n",
    "\n",
    "    # Reconvert all the potential source points\n",
    "    source = np.add(centerlist, potential_sources) # Width, Depth, Length\n",
    "\n",
    "    # Recenter the S2 Source\n",
    "    S_source = np.add(centerlist, np.array([-0.09080822, -0.03022343,  0.02206185]))\n",
    "\n",
    "    # return the potential S2 source and the S2 source\n",
    "    return source, S_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \"\"\" Performs the main script using all the sound data, specific speed of sound, \n",
    "        the names of the distance of arrival methods, and number of combinations of microphone pairs. \"\"\"\n",
    "    \n",
    "    # Speed of sound\n",
    "    sound_speed = 30\n",
    "    \n",
    "    # Number of Pairs of Microphone Combinations\n",
    "    combinations_number = 3\n",
    "    \n",
    "    # Data: Number of Cycles for each Sound Source\n",
    "    # DEBUG:\n",
    "    cycles = ['Cycle'+str(i) for i in range(2)] # for i in range(24)\n",
    "    soundSources = ['S'+str(i) for i in range(2,3)] \n",
    "    sound_list = [soundSource+'_'+cycle for soundSource in soundSources for cycle in cycles]\n",
    "    \n",
    "    # When to find S1 and S2\n",
    "    S1_bool = True\n",
    "    \n",
    "    for source_name in sound_list:\n",
    "            \n",
    "        # Check if name is S2\n",
    "        if source_name in ['S'+str(i)+'_'+'Cycle'+str(j) for i in range(2,3) for j in range(24)]:\n",
    "            S1_bool = False\n",
    "\n",
    "        # Get the Sound Data\n",
    "        sound_data = get_data(source_name)\n",
    "        \n",
    "        # Get the final results via testing all the algorithms available through multithreading\n",
    "        # srp_thread = Thread(target=main, args=(sound_speed, 'SRP', sound_data, combinations_number, S1_bool, source_name))\n",
    "        music_thread = Thread(target=main, args=(sound_speed, 'MUSIC', sound_data, combinations_number, S1_bool, source_name))\n",
    "        # tops_thread = Thread(target=main, args=(sound_speed, 'TOPS', sound_data, combinations_number, S1_bool, source_name))\n",
    "\n",
    "        # srp_thread.start()\n",
    "        music_thread.start()\n",
    "        # tops_thread.start()\n",
    "\n",
    "        # srp_thread.join()\n",
    "        music_thread.join()\n",
    "        # tops_thread.join()\n",
    "                        \n",
    "    print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
